{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdyOWAop9xLw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Questions\n",
        "1. What is Simple Linear Regression ?\n",
        "- Simple Linear Regression is a basic statistical technique used to model the relationship between two continuous variables:\n",
        "\n",
        "One independent variable (X) – also called the predictor or explanatory variable.\n",
        "\n",
        "One dependent variable (Y) – also called the response or outcome variable.\n",
        "2. What are the key assumptions of Simple Linear Regression ?\n",
        "- The key assumptions of Simple Linear Regression (SLR) are crucial for ensuring the validity of the model’s results.\n",
        "1. Linearity 2. Independence of Errors\n",
        "3. Homoscedasticity (Constant Variance of Errors)\n",
        "4. Normality of Errors\n",
        "3. - What does the coefficient m represent in the equation Y=mX+c ?\n",
        "- In the linear equation Y = mX + c, the coefficient\n",
        "𝑚\n",
        "m represents the slope (or gradient) of the line.\n",
        "\n",
        "𝑚\n",
        "m tells you:\n",
        "It indicates how much Y changes for a one-unit change in X.\n",
        "\n",
        "Mathematically, it's the rate of change of Y with\n",
        "respect to X.\n",
        "4. What does the intercept c represent in the equation Y=mX+c ?\n",
        "- In the linear equation:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        "Y=mX+c\n",
        "the intercept\n",
        "𝑐\n",
        "c represents the value of\n",
        "𝑌\n",
        "Y when\n",
        "𝑋\n",
        "=\n",
        "0\n",
        "X=0. In other words, it's the point where the line crosses the Y-axis on a graph.\n",
        "\n",
        "Breakdown:\n",
        "𝑌\n",
        "Y: Dependent variable (output)\n",
        "\n",
        "𝑋\n",
        "X: Independent variable (input)\n",
        "\n",
        "𝑚\n",
        "m: Slope of the line (how much\n",
        "𝑌\n",
        "Y changes for each unit increase in\n",
        "𝑋\n",
        "X)\n",
        "\n",
        "𝑐\n",
        "c: Y-intercept (value of\n",
        "𝑌\n",
        "Y when\n",
        "𝑋\n",
        "=\n",
        "0\n",
        "X=0)\n",
        "5.  How do we calculate the slope m in Simple Linear Regression ?\n",
        "- In Simple Linear Regression, we model the relationship between an independent variable\n",
        "𝑥\n",
        "x and a dependent variable\n",
        "𝑦\n",
        "y using a straight line:\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression ?\n",
        "- The purpose of the least squares method in Simple Linear Regression is to find the best-fitting straight line (regression line) through a set of data points by minimizing the sum of the squared differences (errors) between the observed values and the values predicted by the line.\n",
        "7. - How is the coefficient of determination (R²) interpreted in Simple Linear Regression ?\n",
        "- n Simple Linear Regression, the coefficient of determination (R²) is a key metric that indicates how well the model explains the variability of the dependent variable (Y) using the independent variable (X).\n",
        "SSR = Sum of Squares of Residuals (unexplained variation)\n",
        "\n",
        "SST = Total Sum of Squares (total variation in Y)\n",
        "\n",
        "\n",
        "8. What is Multiple Linear Regression ?\n",
        "- Multiple Linear Regression (MLR) is a statistical technique used to model the relationship between one dependent variable and two or more independent (or explanatory) variables. It extends simple linear regression, which deals with only one independent variable.\n",
        "9. What is the main difference between Simple and Multiple Linear Regression ?\n",
        "- The main difference between Simple Linear Regression and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable.\n",
        "\n",
        "Simple Linear Regression\n",
        "One independent variable (predictor).\n",
        "\n",
        "Equation:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜀\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+ε\n",
        "Example: Predicting a person's weight (Y) based on their height (X).\n",
        "\n",
        "Multiple Linear Regression\n",
        "Two or more independent variables.\n",
        "10. What are the key assumptions of Multiple Linear Regression ?\n",
        "- Multiple Linear Regression (MLR) relies on several key assumptions to produce valid and reliable results. Violations of these assumptions can lead to biased, inefficient, or misleading estimates.\n",
        "1. Linearity\n",
        "Assumption: The relationship between the independent variables and the dependent variable is linear.\n",
        "\n",
        "Implication: The model correctly captures the effect of each predictor as additive and linear.\n",
        "\n",
        "2. Independence of Errors\n",
        "Assumption: The residuals (errors) are independent of each other.\n",
        "\n",
        "Implication: No autocorrelation exists, especially important in time-series data.\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression mode ?\n",
        "- Heteroscedasticity occurs when the variance of the errors (residuals) in a regression model is not constant across all levels of the independent variables. In other words, the spread or \"noise\" in the residuals changes depending on the value of one or more predictors.\n",
        "\n",
        "Homoscedasticity means constant variance of errors (ideal case).\n",
        "\n",
        "Heteroscedasticity means non-constant variance of errors.\n",
        "\n",
        "You can often spot heteroscedasticity by plotting residuals vs. predicted values — if the residuals fan out or funnel in rather than forming a horizontal band, that’s a sign.\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity ?\n",
        "- Great question! High multicollinearity in a Multiple Linear Regression (MLR) model means that some predictor variables are highly correlated with each other. This can make coefficient estimates unstable and inflate their standard errors, which hurts interpretability and predictive performance.\n",
        "1. Remove or Combine Variables\n",
        "Remove one or more highly correlated predictors: If two variables provide redundant information, dropping one can reduce multicollinearity.\n",
        "\n",
        "Combine correlated variables: Create a composite variable (e.g., sum, average, or principal components) that captures the shared information.\n",
        "\n",
        "2. Use Dimensionality Reduction Techniques\n",
        "Principal Component Analysis (PCA): Transform correlated predictors into a smaller set of uncorrelated components.\n",
        "\n",
        "Partial Least Squares (PLS): Similar to PCA but focuses on components that are predictive of the response variable.\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models ?\n",
        "- Great question! When using categorical variables in regression models, you generally need to transform them into a numerical format because most regression algorithms expect numeric input. Here are some common techniques for transforming categorical variables:\n",
        "\n",
        "1. One-Hot Encoding (Dummy Variables)\n",
        "What: Converts each category into a binary column (0/1).\n",
        "\n",
        "Use case: When categories are nominal (no natural order).\n",
        "\n",
        "Example: A \"Color\" variable with values {Red, Blue, Green} becomes three binary columns: Color_Red, Color_Blue, Color_Green.\n",
        "14. What is the role of interaction terms in Multiple Linear Regression ?\n",
        "- Great question! In Multiple Linear Regression (MLR), interaction terms capture the combined effect of two or more predictor variables on the response variable that isn’t simply additive.\n",
        "Capture combined effects: Sometimes the effect of one predictor on the outcome depends on the level of another predictor. For example, the impact of fertilizer on crop yield might depend on the amount of water.\n",
        "\n",
        "Improve model fit: Including interaction terms can improve the explanatory power of the model if these joint effects exist.\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
        "- Simple Linear Regression\n",
        "Model:\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "+\n",
        "𝜖\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x+ϵ\n",
        "\n",
        "Intercept (\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        " ) interpretation:\n",
        "The intercept is the expected value of the dependent variable\n",
        "𝑦\n",
        "y when the predictor\n",
        "𝑥\n",
        "=\n",
        "0\n",
        "x=0.\n",
        "In other words, it’s the point where the regression line crosses the\n",
        "𝑦\n",
        "y-axis.\n",
        "\n",
        "Multiple Linear Regression\n",
        "Model:\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑝\n",
        "𝑥\n",
        "𝑝\n",
        "+\n",
        "𝜖\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "p\n",
        "​\n",
        " x\n",
        "p\n",
        "​\n",
        " +ϵ\n",
        "\n",
        "Intercept (\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        " ) interpretation:\n",
        "The intercept is the expected value of\n",
        "𝑦\n",
        "y when all predictors\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑝\n",
        "=\n",
        "0\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "p\n",
        "​\n",
        " =0.\n",
        "This means the intercept represents the baseline level of\n",
        "𝑦\n",
        "y when every independent variable is zero simultaneously.\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions ?\n",
        "Direction of relationship:\n",
        "\n",
        "If the slope is positive, the dependent variable tends to increase as the independent variable increases.\n",
        "\n",
        "If the slope is negative, the dependent variable tends to decrease as the independent variable increases.\n",
        "\n",
        "Strength of relationship:\n",
        "\n",
        "The magnitude (absolute value) of the slope indicates how strongly the predictor influences the outcome. A larger absolute value means a bigger change in the outcome for each unit change in the predictor.\n",
        "17.  How does the intercept in a regression model provide context for the relationship between variables ?\n",
        "- Baseline value:\n",
        "The intercept tells you the expected value of the outcome variable\n",
        "𝑦\n",
        "y when all predictor variables are zero. This is the baseline level of\n",
        "𝑦\n",
        "y before considering the influence of\n",
        "𝑥\n",
        "x.\n",
        "\n",
        "Meaningful starting point:\n",
        "If\n",
        "𝑥\n",
        "=\n",
        "0\n",
        "x=0 is meaningful in the context of the problem, the intercept shows the starting point of\n",
        "𝑦\n",
        "y. For example, in a model predicting weight gain based on hours of exercise, the intercept gives the expected weight gain when exercise is zero.\n",
        "\n",
        "Interpreting shifts:\n",
        "The intercept helps to understand how much\n",
        "𝑦\n",
        "y shifts away from zero at the baseline before considering other variables. It contextualizes the slope (\n",
        "𝛽\n",
        "1\n",
        "β\n",
        "1\n",
        "​\n",
        " ), which measures change in\n",
        "𝑦\n",
        "y per unit change in\n",
        "𝑥\n",
        "x.\n",
        "18. What are the limitations of using R² as a sole measure of model performance ?\n",
        "- Great question! R² (coefficient of determination) is commonly used to evaluate how well a regression model fits the data, but relying solely on R² to assess model performance has several limitations:\n",
        "\n",
        "Doesn’t indicate causation or correctness\n",
        "R² only measures the proportion of variance explained by the model—it doesn’t tell you if the model is actually correct or meaningful in terms of cause-effect relationships.\n",
        "\n",
        "Can be artificially inflated\n",
        "Adding more predictors to a model, even irrelevant ones, will never decrease R² and often increases it, which can give a misleading sense of improvement. Adjusted R² is better for penalizing unnecessary predictors.\n",
        "19. How would you interpret a large standard error for a regression coefficien ?\n",
        "- A large standard error for a regression coefficient generally indicates greater uncertainty or variability in the estimate of that coefficient. Here’s what that means in more detail:\n",
        "\n",
        "Imprecise Estimate:\n",
        "The coefficient estimate is not very precise; the true value could vary widely around the estimated coefficient.\n",
        "\n",
        "Wide Confidence Interval:\n",
        "Because the standard error is large, the confidence interval for the coefficient will be wide, meaning less confidence in the exact size of the effect.\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
        "- Look at the spread (variance) of residuals across the range of predicted values or an independent variable.\n",
        "\n",
        "If residuals fan out or funnel in, meaning the spread of residuals increases or decreases as the predicted values increase, this suggests heteroscedasticity.\n",
        "\n",
        "Specifically, in a residual plot (residuals on the y-axis, predicted values or independent variable on the x-axis), instead of a random “cloud” of points with constant spread, you might see:\n",
        "\n",
        "A cone shape where residuals get larger (or smaller) as values increase.\n",
        "\n",
        "Patterns like a megaphone, funnel, or systematic variance changes.\n",
        "21. - What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R2 ?\n",
        "-\n",
        "R² measures the proportion of variance in the dependent variable explained by the independent variables.\n",
        "\n",
        "Adjusted R² adjusts R² for the number of predictors in the model, penalizing for adding variables that don’t improve the model much.\n",
        "22. Why is it important to scale variables in Multiple Linear Regression ?\n",
        "- Improves Numerical Stability and Convergence\n",
        "When variables have vastly different scales (e.g., one variable ranges from 0 to 1, another from 1,000 to 10,000), the optimization algorithms (like gradient descent) used to fit the regression model can have trouble converging efficiently. Scaling helps the algorithm run more smoothly and faster.\n",
        "\n",
        "Makes Coefficients More Comparable\n",
        "Without scaling, the regression coefficients correspond to changes in variables measured in very different units, so it’s hard to compare their relative importance. Scaling puts variables on a similar scale, so the size of coefficients better reflects the strength of their influence.\n",
        "\n",
        "23. - What is polynomial regression ?\n",
        "- Polynomial regression is a type of regression analysis where the relationship between the independent variable\n",
        "𝑥\n",
        "x and the dependent variable\n",
        "𝑦\n",
        "y is modeled as an\n",
        "𝑛\n",
        "nth degree polynomial.\n",
        "\n",
        "\n",
        "Instead of fitting a straight line (like in simple linear regression), polynomial regression fits a curve.\n",
        "\n",
        "The model looks like this for a polynomial of degree\n",
        "𝑛\n",
        "n:\n",
        "24. How does polynomial regression differ from linear regression ?\n",
        "- Linear Regression:\n",
        "\n",
        "The model assumes a linear relationship between the input variable(s) and the output.\n",
        "\n",
        "The equation looks like:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "+\n",
        "𝜖\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x+ϵ\n",
        "Here,\n",
        "𝑦\n",
        "y is the dependent variable,\n",
        "𝑥\n",
        "x is the independent variable,\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  is the intercept,\n",
        "𝛽\n",
        "1\n",
        "β\n",
        "1\n",
        "​\n",
        "  is the slope, and\n",
        "𝜖\n",
        "ϵ is the error term.\n",
        "\n",
        "Polynomial Regression:\n",
        "\n",
        "The model assumes a non-linear relationship between the independent variable and dependent variable by including polynomial terms of the input variable.\n",
        "\n",
        "The equation looks like:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑥\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x+β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " x\n",
        "3\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        " +ϵ\n",
        "Here,\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "𝑥\n",
        "3\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "2\n",
        " ,x\n",
        "3\n",
        " ,…,x\n",
        "n\n",
        "  are polynomial terms that allow the curve to fit more complex patterns.\n",
        "  25. When is polynomial regression used ?\n",
        "  - Non-linear trends:\n",
        "When the data shows a curved pattern (e.g., U-shaped or inverted U-shaped), polynomial regression can capture that curvature better than simple linear regression.\n",
        "\n",
        "Better fit for complex relationships:\n",
        "If a linear model underfits the data (poorly models the pattern), a polynomial model can provide a better fit by including higher-degree terms (like\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "𝑥\n",
        "3\n",
        "x\n",
        "2\n",
        " ,x\n",
        "3\n",
        " , etc.).\n",
        "\n",
        "When interactions of predictors are polynomial in nature:\n",
        "For example, if the effect of the predictor on the response grows or diminishes at an increasing rate, polynomial terms can model that.\n",
        "\n"
      ],
      "metadata": {
        "id": "v24zq2Ob93f_"
      }
    }
  ]
}